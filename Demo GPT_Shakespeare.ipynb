{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Building a GPT\n",
        "\n",
        "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
      ],
      "metadata": {
        "id": "wJpXpmjEYC_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading Shakespeare dataset"
      ],
      "metadata": {
        "id": "OVlIWpnbucvk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5hjCcLDr2WC",
        "outputId": "5d059713-b73e-4a3d-ce3c-3e3b45026eb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-27 05:19:49--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-10-27 05:19:49 (17.2 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    shakespeare_text = f.read()\n"
      ],
      "metadata": {
        "id": "qRYRe0RvusXs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "qR9NY3GFUL3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "c4a8a3c1-3e2b-47a2-8cc9-9887f848759b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n",
            "step 0: train loss 4.4116, val loss 4.4022\n",
            "step 100: train loss 2.6568, val loss 2.6670\n",
            "step 200: train loss 2.5091, val loss 2.5060\n",
            "step 300: train loss 2.4199, val loss 2.4339\n",
            "step 400: train loss 2.3508, val loss 2.3567\n",
            "step 500: train loss 2.2965, val loss 2.3129\n",
            "step 600: train loss 2.2413, val loss 2.2503\n",
            "step 700: train loss 2.2057, val loss 2.2196\n",
            "step 800: train loss 2.1637, val loss 2.1870\n",
            "step 900: train loss 2.1248, val loss 2.1509\n",
            "step 1000: train loss 2.1024, val loss 2.1294\n",
            "step 1100: train loss 2.0702, val loss 2.1189\n",
            "step 1200: train loss 2.0386, val loss 2.0808\n",
            "step 1300: train loss 2.0240, val loss 2.0640\n",
            "step 1400: train loss 1.9910, val loss 2.0359\n",
            "step 1500: train loss 1.9699, val loss 2.0304\n",
            "step 1600: train loss 1.9628, val loss 2.0476\n",
            "step 1700: train loss 1.9390, val loss 2.0130\n",
            "step 1800: train loss 1.9080, val loss 1.9934\n",
            "step 1900: train loss 1.9099, val loss 1.9897\n",
            "step 2000: train loss 1.8827, val loss 1.9912\n",
            "step 2100: train loss 1.8718, val loss 1.9743\n",
            "step 2200: train loss 1.8612, val loss 1.9625\n",
            "step 2300: train loss 1.8557, val loss 1.9524\n",
            "step 2400: train loss 1.8439, val loss 1.9470\n",
            "step 2500: train loss 1.8150, val loss 1.9408\n",
            "step 2600: train loss 1.8259, val loss 1.9367\n",
            "step 2700: train loss 1.8112, val loss 1.9332\n",
            "step 2800: train loss 1.8037, val loss 1.9195\n",
            "step 2900: train loss 1.8021, val loss 1.9291\n",
            "step 3000: train loss 1.7931, val loss 1.9165\n",
            "step 3100: train loss 1.7680, val loss 1.9184\n",
            "step 3200: train loss 1.7525, val loss 1.9078\n",
            "step 3300: train loss 1.7594, val loss 1.9048\n",
            "step 3400: train loss 1.7575, val loss 1.8967\n",
            "step 3500: train loss 1.7371, val loss 1.8957\n",
            "step 3600: train loss 1.7269, val loss 1.8934\n",
            "step 3700: train loss 1.7283, val loss 1.8849\n",
            "step 3800: train loss 1.7173, val loss 1.8925\n",
            "step 3900: train loss 1.7219, val loss 1.8790\n",
            "step 4000: train loss 1.7155, val loss 1.8617\n",
            "step 4100: train loss 1.7138, val loss 1.8815\n",
            "step 4200: train loss 1.7032, val loss 1.8664\n",
            "step 4300: train loss 1.7002, val loss 1.8489\n",
            "step 4400: train loss 1.7064, val loss 1.8677\n",
            "step 4500: train loss 1.6916, val loss 1.8577\n",
            "step 4600: train loss 1.6867, val loss 1.8355\n",
            "step 4700: train loss 1.6821, val loss 1.8437\n",
            "step 4800: train loss 1.6676, val loss 1.8412\n",
            "step 4900: train loss 1.6721, val loss 1.8406\n",
            "step 4999: train loss 1.6609, val loss 1.8204\n",
            "\n",
            "\n",
            "KING RICHARD II:\n",
            "Shal love thrance, my becale.\n",
            "Stirranst:\n",
            "What use art that us his vetry, dilth ane away, my fears'd unzormun\n",
            "Your proof in heart my would but\n",
            "With ensengmin latess in ov the doest none:\n",
            "Will joy!\n",
            "All in you little me now by: turn arespent you:\n",
            "Mistrenced peterives; but you, like again Willond,\n",
            "I prings to these rivish encere strooks\n",
            "And strune kindn, rupt for to imper must with all on,\n",
            "That Pried my of.\n",
            "\n",
            "HENRY BOLINGS:\n",
            "You ards bring Edward?\n",
            "\n",
            "GRUKERS OUCERD\n",
            "HERRY MaNCENTIO:\n",
            "Now, no to Pome, grithout, which have have mysel-sporrow the men.\n",
            "Was strocks, in with the countrus much fried:\n",
            "Whiltchs lom my is sontluced themseled.\n",
            "\n",
            "AUTOLYCUMELLA:\n",
            "Cass, to their meen; boye stings his refess, as not come,\n",
            "Over unnish, no duke in, more they throbed.\n",
            "\n",
            "OXFORD:\n",
            "Encews my Tolevy buch not, to so lack.\n",
            "\n",
            "PRINIUS:\n",
            "He done,-hile.\n",
            "\n",
            "KING HENRY GARD II:\n",
            "Whey\n",
            "seven at is is onter now the sut now.\n",
            "\n",
            "CAGARET:\n",
            "Be that the must percless kings to hy mear\n",
            "fathir watch thus wout\n",
            "Shall us you contomer, I am, it we sofleign.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "What how ser tows to Eams:\n",
            "Way no surring of, mastent,\n",
            "The kill worting this sonvilger:\n",
            "How would, chill friend! all beenIn\n",
            "Before and to the severeign begk.\n",
            "\n",
            "LUCINTIO:\n",
            "Outh true exre breamn'd my have eath I disht;\n",
            "But that's let where perflels,\n",
            "Have my oldress tojber the vown;\n",
            "Mad there'e hears still it is stide,\n",
            "All treest not they since as beguare.\n",
            "\n",
            "QUEENb, thRENCE:\n",
            "Well, that sweet;\n",
            "Was his nend to me, meer, speak.\n",
            "\n",
            "LUCHIUSTER:\n",
            "I have tarrangule, we\n",
            "ell smes of my soul be\n",
            "the shaltesss; for but olk virty\n",
            "The but then etends upon, Burwirds.\n",
            "\n",
            "COMINIUS:\n",
            "What is shame, arling\n",
            "Nor turnity's trues so manst unarwild-Garriaggive,\n",
            "Befter, my living as Coling\n",
            "That the have bloy not, there wife it:\n",
            "The that heave out citt, one the linour?\n",
            "\n",
            "QUEEN PETOLY:\n",
            "Why.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Ney honer, Go life?\n",
            "\n",
            "COMINIUS:\n",
            "You hovesse would than with say, thy promble. I was nseat,\n",
            "But brought God to, Jurious and you briings men's from thee such I he had night,\n",
            "\n",
            "Marr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation"
      ],
      "metadata": {
        "id": "DuIv48kmUOC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQKXWCxjnPCh",
        "outputId": "55f7f092-4063-41b0-cfba-eb38e3004d63"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Be empation, that his lost to betchsed in bently that go me,\n",
            "so the dead must have togentry thurt\n",
            "And is took a wall the late of Oportence?\n",
            "The furets\n",
            "Thou that switise: why death! God not for mindtward.\n",
            "\n",
            "LORDY BOLINGHARD:\n",
            "What now, preed that you goent that clamed,\n",
            "But then unter some soass\n",
            "To Earmadiney, may to in\n",
            "no approuble ild.\n",
            "\n",
            "DUKE OW OF RIVERLIZA:\n",
            "Ay, a my slent you do hellows.\n",
            "\n",
            "DUKEN MARCULET:\n",
            "Lare's, I shome, movothin his will;\n",
            "I that dese, God, hor densless not testruct;--forcesser befound,\n",
            "In lave brath, not a swear to beight guit, mestray.\n",
            "It spy womans, I was, nobly heartn'd;\n",
            "that for that shall to: where his norber rump matter of I have slept for you padty me were is.\n",
            "\n",
            "GREMOMIO:\n",
            "And oletter, Citize,\n",
            "The wompy sir, ark what steed mister that bit.\n",
            "\n",
            "COMINIUS:\n",
            "I to\n",
            "Am gatest what his father alling\n",
            "Cimtaing bloided. That is thuntring not\n",
            "that of bence Warwich.\n",
            "you hast with stomper. Corn;\n",
            "Privorther, grave criuss you'll'd\n",
            "Do some dish him sheptred breings so his\n",
            "Tyan havist shonour; whose his blout\n",
            "these manysts that know joys, sits insmy beguin all their world cause:\n",
            "Outh ware wiltarking:'\n",
            "But, or thou know, God driars; thereforiol'd\n",
            "Gham to melseed, horse ormile, these love;\n",
            "That I love kim, whom who you;\n",
            "My profe his woel? such, whance to mestraidy\n",
            "To frown'd ifterlence; bust like untymenter it.\n",
            "\n",
            "CA&Majolinal,\n",
            "Who, as sorrow? there watch you no prame,\n",
            "But Parch? is Hercy wild, no deatess,\n",
            "And pliunated's pole wods go ot cagainxpost.\n",
            "\n",
            "MARGARET:\n",
            "Congur, Stay, I bulI nothing fralkingner:\n",
            "O, ere is bons. I wasged a devisitagently, by Looke or saves.\n",
            "\n",
            "KING RICHARD III:\n",
            "Warwich him to ve win thy lovings.\n",
            "\n",
            "MENENENIUS:\n",
            "My asskness todure;\n",
            "be thurse fairst it sBut your frieht\n",
            "Of is thou that sumides. But, cold stronge it,\n",
            "And nex strue temper, stretisbit! revolion what,\n",
            "what thou sona wergunce!\n",
            "Her loved, to sureed or you contry.\n",
            "\n",
            "HENRWIMHING HENRY VI:\n",
            "How thou here thou country, he straige, set:\n",
            "To\n",
            "Riven of us.\n",
            "\n",
            "BUCKINGHARD II:\n",
            "What where had\n",
            "with; such will w\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"James Bond\"\n",
        "context = torch.tensor(encode(prompt), dtype=torch.long, device=device).view(1, -1)\n",
        "print(decode(m.generate(context, max_new_tokens=200)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEm5V1QvwDDS",
        "outputId": "5f40d56d-1e6e-48a9-ea18-6c9434a811d6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "James Bondemagtium or forful pach,\n",
            "not you was our set to my virreigly,\n",
            "And by this beins! what pety your worst\n",
            "TRavost baster shall screts I some.\n",
            "I is All thee then was to yet, my may, his.\n",
            "\n",
            "PUFIDBY:\n",
            "In\n",
            "There\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(m.state_dict(), 'bigram_language_model.pth')"
      ],
      "metadata": {
        "id": "fjjvMifYZf7x"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "loaded_model = BigramLanguageModel()  # Initialize an instance of your model\n",
        "loaded_model.load_state_dict(torch.load('bigram_language_model.pth'))\n",
        "loaded_model.to(device).eval()  # Set the model to evaluation mode\n"
      ],
      "metadata": {
        "id": "XDlaUiT73Hci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f747796f-5aa0-48e0-b79e-4152fd23b714"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BigramLanguageModel(\n",
              "  (token_embedding_table): Embedding(65, 64)\n",
              "  (position_embedding_table): Embedding(32, 64)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=64, out_features=65, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(loaded_model.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "HAa7CQP34CiH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c70b29a4-1d68-4ef8-c303-0e9cc2135e79"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ScERLANUS:\n",
            "Thy Pount, sile that brrosfrenger: why!\n",
            "To iccare, and toght; he basidy not.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I contrumptrian will depear in lifedge,\n",
            "And there such but this this is gury,\n",
            "Coter must but ood my was be our on\n",
            "Both loves your wonds it hate.\n",
            "\n",
            "LUCIO:\n",
            "Thy.\n",
            "\n",
            "PETUTUM:\n",
            "Nor renssul.\n",
            "\n",
            "KING RICHARD:\n",
            "Marched How rathroth:\n",
            "I enterry what's give I wark, how is Volderbard;\n",
            "So shall tyeet, sempargary God that was wetters,\n",
            "As tell subjetst me. To us.\n",
            "\n",
            "MENENIUS:\n",
            "What a watcrower\n",
            "throne maken was that whose dough of his no are\n",
            "Jude.\n",
            "\n",
            "KING Ricly my wrosest seem adling.\n",
            "\n",
            "MENENIUS:\n",
            "Madiery, you nou!\n",
            "\n",
            "Senvile.\n",
            "Out him ply in with on to ear worlds as his forther?\n",
            "Do shall sail merpon and with she cittite supring fries senighteing to the Towers chis tides,\n",
            "Good for youth that,' gracced: or I didons thre a they joy;\n",
            "What is in that Rivattars, that I give to own\n",
            "bear must you burldiers honour than\n",
            "-fathluty alls tone that ched;--terrat our suchip colditore,\n",
            "Nor calmes of the dauge that king,\n",
            "O bring of sidel fews.\n",
            "\n",
            "MONTANUS:\n",
            "I whome my kingered\n",
            "God, with the glest dear to stindny,\n",
            "But thou were thou pary nows:\n",
            "What mad, child mest some of them.\n",
            "\n",
            "POMPEY:\n",
            "Would advanter me;\n",
            "I was If in only ghith, life in his will not be the plaspey that had in pliage.\n",
            "DUKE OVEN MARDWIU:\n",
            "Comitionuit, our mentors:\n",
            "And then on.\n",
            "\n",
            "KING RICHARD II:\n",
            "What his was sure volive hone of togly viturement thou beting to come,\n",
            "Sithout, my lifed to be with the than maden;\n",
            "Rimans foold--lurshmed and ring,\n",
            "And clare in my herse other not man,\n",
            "That into them nury of anTher your blood his goeld watch,\n",
            "In will my plot press of behold.\n",
            "\n",
            "P$ALINGHARG:\n",
            "What Ishom me.\n",
            "\n",
            "GLOUCESTER:\n",
            "God then, why his hold, beting now streath as matty thhat be the wish.\n",
            "New us.\n",
            "\n",
            "KING RICHARD II:\n",
            "Yea, peast?\n",
            "\n",
            "POLINGE:\n",
            "Be the patch, suchip the this withon friendder,\n",
            "Petitce tumners I holde; then aghne;\n",
            "Meast in throok, take's so mast slave in great his. Gut a what a do must make that where carried of is as:\n",
            "While own stelligner, clonge gsherven i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sU0jgBUE4H1W"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}