{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Building a GPT\n",
        "\n",
        "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
      ],
      "metadata": {
        "id": "wJpXpmjEYC_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading Wikipedia dataset from Kaggle"
      ],
      "metadata": {
        "id": "Vrc0I031uhsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "0qOa_K7Te3WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "EWRZ9JuqgLSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "qQvSdDu6gO03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d mikeortman/wikipedia-sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRJrk6GKeoht",
        "outputId": "5a9731dd-5fc1-405a-8f17-9d2cc40fcf23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading wikipedia-sentences.zip to /content\n",
            " 99% 312M/314M [00:10<00:00, 36.0MB/s]\n",
            "100% 314M/314M [00:10<00:00, 32.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/wikipedia-sentences.zip' -d '/content'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DljQswPIgoPc",
        "outputId": "f91ac4f0-53d6-42a2-c8cc-7207becaeb61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/wikipedia-sentences.zip\n",
            "  inflating: /content/wikisent2.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "\n",
        "DATA_PATH = 'wikisent2.txt'\n",
        "\n",
        "# load wikipedia sentences\n",
        "with open(DATA_PATH, 'r') as f:\n",
        "    lines = f.read().splitlines()\n",
        "\n",
        "print('Total Lines:', len(lines))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy9N_WHBdxrL",
        "outputId": "a9c9a8e9-15dc-4856-cbf3-bb755b0e6533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Lines: 7871825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting 250k lines from the dataset.\n",
        "random.seed(42)\n",
        "texts = random.choices(lines, k=250000)\n",
        "del lines"
      ],
      "metadata": {
        "id": "QuKYqz5Hdxom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = re.sub('@.*?\\s+', '', text)  # Remove mentions\n",
        "    text = re.sub('#.*?\\s+', '', text)  # Remove hashtags\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^\\w\\s\\'.]', '', text)  # Remove special characters except for single quotes and periods\n",
        "    text = re.sub('\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "    text = re.sub('^\\d+\\s*|^\\d+\\.\\d+\\s*|^\\d+\\.\\d+\\.\\d+\\s*', '', text)  # Remove digits at the start of sentences\n",
        "    text = text.strip()  # Remove leading and trailing whitespace\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "QJZ9EbXTdxl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_text = [preprocess(t) for t in texts]"
      ],
      "metadata": {
        "id": "xwQY2DZadxi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select dataset"
      ],
      "metadata": {
        "id": "pW8QKA0NkVKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select which dataset to be used for training GPT model\n",
        "text = '\\n'.join(wiki_text)"
      ],
      "metadata": {
        "id": "iH331e27vt2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "qR9NY3GFUL3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "ad0a7d2a-6c2a-4e42-a5aa-d534766dac40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209987 M parameters\n",
            "step 0: train loss 4.3875, val loss 4.3896\n",
            "step 100: train loss 2.6774, val loss 2.6653\n",
            "step 200: train loss 2.5536, val loss 2.5541\n",
            "step 300: train loss 2.4927, val loss 2.5000\n",
            "step 400: train loss 2.4414, val loss 2.4388\n",
            "step 500: train loss 2.3893, val loss 2.3936\n",
            "step 600: train loss 2.3322, val loss 2.3375\n",
            "step 700: train loss 2.2821, val loss 2.2870\n",
            "step 800: train loss 2.2551, val loss 2.2483\n",
            "step 900: train loss 2.2171, val loss 2.2173\n",
            "step 1000: train loss 2.1899, val loss 2.2021\n",
            "step 1100: train loss 2.1680, val loss 2.1555\n",
            "step 1200: train loss 2.1539, val loss 2.1524\n",
            "step 1300: train loss 2.1192, val loss 2.1215\n",
            "step 1400: train loss 2.1009, val loss 2.1102\n",
            "step 1500: train loss 2.0867, val loss 2.0982\n",
            "step 1600: train loss 2.0817, val loss 2.0922\n",
            "step 1700: train loss 2.0578, val loss 2.0799\n",
            "step 1800: train loss 2.0483, val loss 2.0522\n",
            "step 1900: train loss 2.0307, val loss 2.0284\n",
            "step 2000: train loss 2.0123, val loss 2.0114\n",
            "step 2100: train loss 1.9998, val loss 2.0010\n",
            "step 2200: train loss 1.9918, val loss 1.9958\n",
            "step 2300: train loss 1.9740, val loss 1.9832\n",
            "step 2400: train loss 1.9832, val loss 1.9877\n",
            "step 2500: train loss 1.9676, val loss 1.9676\n",
            "step 2600: train loss 1.9455, val loss 1.9583\n",
            "step 2700: train loss 1.9598, val loss 1.9549\n",
            "step 2800: train loss 1.9336, val loss 1.9330\n",
            "step 2900: train loss 1.9349, val loss 1.9357\n",
            "step 3000: train loss 1.9320, val loss 1.9268\n",
            "step 3100: train loss 1.9118, val loss 1.9172\n",
            "step 3200: train loss 1.8985, val loss 1.9170\n",
            "step 3300: train loss 1.8961, val loss 1.9016\n",
            "step 3400: train loss 1.9017, val loss 1.9049\n",
            "step 3500: train loss 1.8814, val loss 1.8848\n",
            "step 3600: train loss 1.8763, val loss 1.8830\n",
            "step 3700: train loss 1.8647, val loss 1.8618\n",
            "step 3800: train loss 1.8778, val loss 1.8758\n",
            "step 3900: train loss 1.8681, val loss 1.8709\n",
            "step 4000: train loss 1.8631, val loss 1.8513\n",
            "step 4100: train loss 1.8404, val loss 1.8563\n",
            "step 4200: train loss 1.8576, val loss 1.8585\n",
            "step 4300: train loss 1.8488, val loss 1.8441\n",
            "step 4400: train loss 1.8456, val loss 1.8528\n",
            "step 4500: train loss 1.8287, val loss 1.8333\n",
            "step 4600: train loss 1.8377, val loss 1.8437\n",
            "step 4700: train loss 1.8415, val loss 1.8391\n",
            "step 4800: train loss 1.8237, val loss 1.8279\n",
            "step 4900: train loss 1.8267, val loss 1.8108\n",
            "step 4999: train loss 1.8083, val loss 1.8028\n",
            "\n",
            "Toperition Mualub Yurg is an June Oction Wasell Roell Charamed born Chreened suound to papubW.\n",
            "Carrch unining is locans xocks intur.\n",
            "Lemdegar Agard's flowed's cengerch the gends of rabimp in the otherling enturating foulling the famion and was serveging turwh letopis in dincored behored woulb was include a member cagend considen.\n",
            "It Sance Polour 30 Jearia pessenived taping deside internation the the marks feaded born Julypics oni's 20050 in the number Decembers.\n",
            "It since at with sunder a south with nowleill sewerlide lead in known Alleted 3 15 hen outher speer of the single for the teenting sall dunkaried taking known in the United Wempocensis and member 89 altabrity foring are the furst the velder or Kalwitar Bak to Sillais as Anshor occomvail and an Gesiundae.\n",
            "Satuskau an incound to Pelong Rovel Yea Humal Housear intersent by guity on the clubs Brousepic Arabome.\n",
            "As more player chosten is a relevision one and a.\n",
            "Austractic iin the poofte retorizatius abirity.\n",
            "The celling own in electure releases who defered suchaily as and savit counded thre Smallle Bruarl Helvel Bycle and the Ahemouncilum film.\n",
            "Albut the enteride at is Canad Suppher's AffinctHlenk and Eaylian Team thaint of the Februe.\n",
            "The seally a centactur legeral pub.\n",
            "The Helgerrab Whrom was for CERA known as amnings.\n",
            "Ambaradcam his is detevollion reformed on Monol Pance fulle Ahilo.\n",
            "Pakollong Fook.\n",
            "It bass Suelz Councalonly who creetary ladia can flystover concels releator carcher been on the avencember the Super from coollo dummined frient and in ECA Tangle Austra.\n",
            "C3 Hynada Bess' is the Neuman.\n",
            "The partopic is a the album of in the Spainius.\n",
            "The Canada's Gerail Spubuls four Itailize eler for taked if a hellabing Oleadian Sucesh and fealled by Le Stistine Munur BHT Dynana's a'Grermalkines.\n",
            "The ircreatus out producate devent and plates in the cammet judine on Reading In Creatar in FCision of clade municns subbited on 201 he India.\n",
            "In she dable of the number the of Mide lockdenth born 1965.\n",
            "The het gining by s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation"
      ],
      "metadata": {
        "id": "DuIv48kmUOC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQKXWCxjnPCh",
        "outputId": "f0c80602-ff76-42ff-84c2-851a340c2a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "An companion theoriso pirted attralized on National Leed T Header Calcas the Ifter and specially beculation incture.\n",
            "He muninal hendel succell tanisted wember a Gug greend of Osekly from Serbutk more whild ampres lungulary in 1940s and centernment to lowested with Leleased Hustral U.Breprent in Danoubagawail Raig Penly Soshas Mockalisberling justom couil a.\n",
            "Mana arnown are 19 to the Cinning Boon.\n",
            "It a delective sequed of it rage from the Pepresist was FLIE for bille on Odemic Commostods and cholod scollage.\n",
            "Thy is is locations Aterns fere early endempions.\n",
            "Hn the Counciture.\n",
            "Warlon Louglor Brikhame Dicar teamment runce as Palip Mouns Lee and Ribber Tour.\n",
            "Wheld and coned when as peaper lated in the secreenk and the stonalition operation bivilled ots Connal PPI in a Gelds Kenam and UNWCEU is a freed hele as entwopled in elignes.\n",
            "It inguites teapeal in EPniciloon the II Aumawau free in Meorish Crastal Marlis Richised Terats Callity introundended andfier frehedenoused in 1962 is confe on reparts at the GII dorida teund along Gerens in Dohzy.\n",
            "Delva Juli SM is a specially Encheracted of 6. Alpil in the water probled on the locame band which prevaged by for Componeced Myephes preclies the Leating in 84 helding Uusentislownix Lame educated Uckar Tanis panage Aunker quilus Marlis.\n",
            "It rantern on the Mhin's Ancemty of the congloped the notballerple corple in the were eince and suastearlemba used in Inmento all was thend serven is hen hibn et arth earlvail degincistly munit.\n",
            "There Goxe Pakelliforn UCEL guilled by const tyllowing ane in the Progleancane in Kintre House Stake Jarner Lynd Euring and who was the elevent regation of an stock and stilf for but one of there whith gropons.\n",
            "The dailwook was released on her case followz produce one on Club Zans Hock Achor.\n",
            "It countapan ham lund 200on.\n",
            "Longeon Ad's estad's specions.\n",
            "The Syre was wrillf.\n",
            "Srieldinger and condrad of inder and coc hee day's dairped to dick in a played gund's sunder form 20 minored and a seconder 70 seased a B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"James Bond\"\n",
        "context = torch.tensor(encode(prompt), dtype=torch.long, device=device).view(1, -1)\n",
        "print(decode(m.generate(context, max_new_tokens=200)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEm5V1QvwDDS",
        "outputId": "5821a559-48b9-4a14-fa52-ee860a5947ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "James Bondoodernsman for County for Hungta Bedemberal a name the city.\n",
            "Querlia and the cannike.\n",
            "The sown mount for the served as actnian Malid and Billade Norali Unjay Campaner.\n",
            "In 0 16 was He efferers operated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(m.state_dict(), 'bigram_language_model.pth')"
      ],
      "metadata": {
        "id": "fjjvMifYZf7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "loaded_model = BigramLanguageModel()  # Initialize an instance of your model\n",
        "loaded_model.load_state_dict(torch.load('bigram_language_model.pth'))\n",
        "loaded_model.to(device).eval()  # Set the model to evaluation mode\n"
      ],
      "metadata": {
        "id": "XDlaUiT73Hci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6fb03da-40ad-4c3d-a75e-88e9ae906fbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BigramLanguageModel(\n",
              "  (token_embedding_table): Embedding(67, 64)\n",
              "  (position_embedding_table): Embedding(32, 64)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=64, out_features=67, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(loaded_model.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "HAa7CQP34CiH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4a80bf1-8051-46f6-af4e-c9bd5b594402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Garrually in Blimsling the Aking Cpiculand.\n",
            "The was mugan and medisor artimes mell that steatly to are bindiling in the functbornst to strumber in firsts now teather in 203 grone for prespons.\n",
            "It is mulate relequePent at epajyshtengers nontorial releatistan as the frecult Coable Was Roug's.\n",
            "Pretusion searies and evepress and inin the County 2016 to and in the coveling diven at the gave the AentARF10 player in trist of pWC in the Cour Chumiry whyretainment chus on electronic primm townspolits and year muning perfillied in rightone angemounted as at the S. Amerimel malle La.3 authinworklis publitic freek in the soldided a stable to a ferific.\n",
            "Heranchoy to peaces had other collong came.\n",
            "Farrand new was networld gensign mader.\n",
            "Pladenama is album the teld an mages fru10 177 the after thrank Buleld on Hen lendlus picce series are scine edalled march one by ter.\n",
            "The 553 highin featured for Vong Kauan Carrosmacila its single deams in admonam.\n",
            "Atalling parliniqued recomed his electribed to pecultured on the southeried.\n",
            "Zan acceopincent fon the Amer Illead and Hadort born sacromich is new the annewilo highelt has gangener genompon or delephensicual munct bailic skeple of the allycence serve for elecring of local allong longworking boughen as hus Tacies belowed in mount caster madisomam and colles ronsiding as county an Tirlanseeual Rheloting.\n",
            "After.5 dured 1990s is as in Eavery approdaced the famion.\n",
            "The Murremonew GTJOS releak was one homed munious the censusporitien and corple Warip in ruture from Ungram album Agrals wamening egronclish norgance carest a enderalled Anterning as ment began ca.\n",
            "The Gertall Collasee and persia in to projeciation 33 and fulliangs by the tums Redy of the film vation and consesfitiliamental national then inter both 60.\n",
            "Water and and Jule Howleor Specie Seriards building in the lope tare's in a Asseclored the Agrinpon election the renum been 17 Olymp Pat Darch is a Publish Afsilo the but was a publim at a dulden are centure all and lemements in th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sU0jgBUE4H1W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}